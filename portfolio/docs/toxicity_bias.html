<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Toxicity & Bias Detection Project</title>
    <link rel="stylesheet" href="docs.css">
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;500;600&display=swap" rel="stylesheet">
</head>
<body>
    <div class="markdown-content">
        <h1>Toxicity & Bias Detection Annotation Project</h1>

        <p>This project provides a Docker-based Doccano setup for multi-label classification of toxic content with bias detection.</p>
        
        <p><strong>GitHub Repository:</strong> <a href="https://github.com/austinLorenzMccoy/Data_annotation/tree/main/toxicity_bias_project" target="_blank">https://github.com/austinLorenzMccoy/Data_annotation/tree/main/toxicity_bias_project</a></p>

        <h2>Features</h2>

        <ul>
            <li>Multi-label classification of toxic content</li>
            <li>Docker-based Doccano setup for scalable annotation</li>
            <li>Real-time validation checks and quality control</li>
            <li>Support for detecting multiple dimensions of toxicity and bias</li>
            <li>Comprehensive annotation guidelines and training materials</li>
        </ul>

        <h2>Technologies Used</h2>

        <ul>
            <li><strong>Doccano</strong>: Primary annotation platform</li>
            <li><strong>Docker</strong>: Containerization for easy deployment</li>
            <li><strong>Python</strong>: Backend processing and analysis</li>
            <li><strong>Pandas</strong>: Data manipulation and statistics</li>
            <li><strong>scikit-learn</strong>: Inter-annotator agreement metrics</li>
        </ul>

        <h2>Annotation Schema</h2>

        <p>The toxicity and bias annotation schema includes:</p>

        <ol>
            <li><strong>Toxicity Categories</strong>:
                <ul>
                    <li>Hate Speech</li>
                    <li>Harassment</li>
                    <li>Profanity</li>
                    <li>Threats</li>
                    <li>Sexual Content</li>
                    <li>Self-Harm</li>
                    <li>Violence</li>
                </ul>
            </li>
            <li><strong>Bias Dimensions</strong>:
                <ul>
                    <li>Gender Bias</li>
                    <li>Racial/Ethnic Bias</li>
                    <li>Religious Bias</li>
                    <li>Age Bias</li>
                    <li>Disability Bias</li>
                    <li>Socioeconomic Bias</li>
                    <li>Political Bias</li>
                </ul>
            </li>
            <li><strong>Severity Levels</strong>:
                <ul>
                    <li>Mild</li>
                    <li>Moderate</li>
                    <li>Severe</li>
                </ul>
            </li>
        </ol>

        <h2>Workflow</h2>

        <ol>
            <li>Content collection and pre-processing</li>
            <li>Initial toxicity screening</li>
            <li>Detailed multi-label annotation in Doccano</li>
            <li>Bias dimension annotation</li>
            <li>Quality control and agreement calculation</li>
            <li>Dataset finalization and export</li>
        </ol>

        <h2>Docker Setup</h2>

        <p>The project includes a Docker-based setup that provides:</p>
        <ul>
            <li>Isolated annotation environment</li>
            <li>Multi-user support with role-based access</li>
            <li>Persistent storage for annotations</li>
            <li>Easy deployment across different environments</li>
            <li>Integrated backup and version control</li>
        </ul>

        <a href="../index.html" class="back-link">Back to Portfolio</a>
    </div>
</body>
</html>
